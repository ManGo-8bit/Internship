{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77b656de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b16977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Header Tags\n",
      "0                       Main Page\n",
      "1            Welcome to Wikipedia\n",
      "2   From today's featured article\n",
      "3                Did you knowÂ ...\n",
      "4                     In the news\n",
      "5                     On this day\n",
      "6      From today's featured list\n",
      "7        Today's featured picture\n",
      "8        Other areas of Wikipedia\n",
      "9     Wikipedia's sister projects\n",
      "10            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "header_text = [tag.get_text() for tag in header_tags]\n",
    "df = pd.DataFrame({'Header Tags': header_text})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "333bf8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "382c8a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage. Status code: 404\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the website\n",
    "url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table containing the list of former presidents\n",
    "presidents_table = soup.find(\"table\", {\"class\": \"table\"})\n",
    "\n",
    "# Create empty lists to store the data\n",
    "names = []b\n",
    "terms_of_office = []\n",
    "\n",
    "# Iterate through the table rows to extract data\n",
    "for row in presidents_table.find_all(\"tr\")[1:]:\n",
    "    columns = row.find_all(\"td\")\n",
    "    name = columns[0].get_text(strip=True)\n",
    "    term_of_office = columns[1].get_text(strip=True)\n",
    "    names.append(name)\n",
    "    terms_of_office.append(term_of_office)\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "data = {\n",
    "    \"Name\": names,\n",
    "    \"Term of Office\": terms_of_office\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb4c0524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6acdc066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams (Men's Cricket):\n",
      "              Team Matches Points Rating\n",
      "0        IndiaIND      49  5,839    119\n",
      "1    AustraliaAUS      36  4,015    112\n",
      "2     PakistanPAK      32  3,525    110\n",
      "3  South AfricaSA      29  3,166    109\n",
      "4   New ZealandNZ      38  4,007    105\n",
      "5      EnglandENG      34  3,377     99\n",
      "6     Sri LankaSL      43  3,943     92\n",
      "7   BangladeshBAN      40  3,574     89\n",
      "8  AfghanistanAFG      26  2,170     83\n",
      "9   West IndiesWI      38  2,582     68\n",
      "Top 10 ODI Batsmen (Men's Cricket):\n",
      "                  Batsman Team Rating\n",
      "0             Babar Azam  PAK    829\n",
      "1           Shubman Gill  IND    823\n",
      "2        Quinton de Kock   SA    769\n",
      "3       Heinrich Klaasen   SA    756\n",
      "4           David Warner  AUS    747\n",
      "5            Virat Kohli  IND    747\n",
      "6           Harry Tector  IRE    729\n",
      "7           Rohit Sharma  IND    725\n",
      "8  Rassie van der Dussen   SA    716\n",
      "9            Imam-ul-Haq  PAK    704\n",
      "Top 10 ODI Bowlers (Men's Cricket):\n",
      "            Bowler Team Rating\n",
      "0  Josh Hazlewood  AUS    670\n",
      "1  Mohammed Siraj  IND    668\n",
      "2  Keshav Maharaj   SA    656\n",
      "3     Rashid Khan  AFG    654\n",
      "4     Trent Boult   NZ    653\n",
      "5   Mohammad Nabi  AFG    641\n",
      "6      Adam Zampa  AUS    635\n",
      "7      Matt Henry   NZ    634\n",
      "8   Kuldeep Yadav  IND    632\n",
      "9  Shaheen Afridi  PAK    625\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "def scrape_icc_rankings(url, ranking_type):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        data = []\n",
    "\n",
    "        if ranking_type == \"teams\":\n",
    "            ranking_table = soup.find(\"table\", {\"class\": \"table\"})\n",
    "            rows = ranking_table.find_all(\"tr\")\n",
    "            rows = rows[1:11] \n",
    "\n",
    "            for row in rows:\n",
    "                columns = row.find_all(\"td\")\n",
    "                team_name = columns[1].get_text(strip=True)\n",
    "                matches = columns[2].get_text(strip=True)\n",
    "                points = columns[3].get_text(strip=True)\n",
    "                rating = columns[4].get_text(strip=True)\n",
    "                data.append([team_name, matches, points, rating])\n",
    "\n",
    "            df = pd.DataFrame(data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "            return df\n",
    "\n",
    "        elif ranking_type == \"batsmen\" or ranking_type == \"bowlers\":\n",
    "            ranking_table = soup.find(\"table\", {\"class\": \"table rankings-table\"})\n",
    "            rows = ranking_table.find_all(\"tr\")\n",
    "            rows = rows[1:11]  \n",
    "\n",
    "            for row in rows:\n",
    "                columns = row.find_all(\"td\")\n",
    "                player_name = columns[1].get_text(strip=True)\n",
    "                team = columns[2].get_text(strip=True)\n",
    "                rating = columns[3].get_text(strip=True)\n",
    "                data.append([player_name, team, rating])\n",
    "\n",
    "            if ranking_type == \"batsmen\":\n",
    "                df = pd.DataFrame(data, columns=[\"Batsman\", \"Team\", \"Rating\"])\n",
    "            else:\n",
    "                df = pd.DataFrame(data, columns=[\"Bowler\", \"Team\", \"Rating\"])\n",
    "            return df\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "odi_teams_url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "odi_batsmen_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "odi_bowlers_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "odi_teams_df = scrape_icc_rankings(odi_teams_url, \"teams\")\n",
    "print(\"Top 10 ODI Teams (Men's Cricket):\\n\", odi_teams_df)\n",
    "odi_batsmen_df = scrape_icc_rankings(odi_batsmen_url, \"batsmen\")\n",
    "print(\"Top 10 ODI Batsmen (Men's Cricket):\\n\", odi_batsmen_df)\n",
    "odi_bowlers_df = scrape_icc_rankings(odi_bowlers_url, \"bowlers\")\n",
    "print(\"Top 10 ODI Bowlers (Men's Cricket):\\n\", odi_bowlers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bf882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fc1e15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams (Women's Cricket):\n",
      "              Team Matches Points Rating\n",
      "0    AustraliaAUS      19  3,084    162\n",
      "1      EnglandENG      23  2,991    130\n",
      "2  South AfricaSA      21  2,446    116\n",
      "3        IndiaIND      18  1,745     97\n",
      "4   New ZealandNZ      21  2,014     96\n",
      "5   West IndiesWI      18  1,610     89\n",
      "6     Sri LankaSL       9    714     79\n",
      "7   BangladeshBAN      11    816     74\n",
      "8     ThailandTHA      11    753     68\n",
      "9     PakistanPAK      21  1,435     68\n",
      "Top 10 ODI Batsmen (Women's Cricket):\n",
      "                 Batsman Team Rating\n",
      "0  Natalie Sciver-Brunt  ENG    807\n",
      "1           Beth Mooney  AUS    750\n",
      "2   Chamari Athapaththu   SL    736\n",
      "3       Laura Wolvaardt   SA    727\n",
      "4       Smriti Mandhana  IND    708\n",
      "5          Alyssa Healy  AUS    698\n",
      "6          Ellyse Perry  AUS    697\n",
      "7      Harmanpreet Kaur  IND    694\n",
      "8           Meg Lanning  AUS    662\n",
      "9        Marizanne Kapp   SA    642\n",
      "Top 10 ODI All-rounders (Women's Cricket):\n",
      "                  Player Team Rating\n",
      "0        Marizanne Kapp   SA    385\n",
      "1      Ashleigh Gardner  AUS    377\n",
      "2  Natalie Sciver-Brunt  ENG    360\n",
      "3       Hayley Matthews   WI    358\n",
      "4           Amelia Kerr   NZ    346\n",
      "5         Deepti Sharma  IND    312\n",
      "6          Ellyse Perry  AUS    282\n",
      "7         Jess Jonassen  AUS    227\n",
      "8         Sophie Devine   NZ    227\n",
      "9              Nida Dar  PAK    224\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "def scrape_womens_odi_rankings(url, ranking_type):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        data = []\n",
    "\n",
    "        if ranking_type == \"teams\":\n",
    "            ranking_table = soup.find(\"table\", {\"class\": \"table\"})\n",
    "            rows = ranking_table.find_all(\"tr\")\n",
    "            rows = rows[1:11]\n",
    "\n",
    "            for row in rows:\n",
    "                columns = row.find_all(\"td\")\n",
    "                team_name = columns[1].get_text(strip=True)\n",
    "                matches = columns[2].get_text(strip=True)\n",
    "                points = columns[3].get_text(strip=True)\n",
    "                rating = columns[4].get_text(strip=True)\n",
    "                data.append([team_name, matches, points, rating])\n",
    "\n",
    "            df = pd.DataFrame(data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "            return df\n",
    "\n",
    "        elif ranking_type == \"batsmen\" or ranking_type == \"allrounders\":\n",
    "            ranking_table = soup.find(\"table\", {\"class\": \"table rankings-table\"})\n",
    "            rows = ranking_table.find_all(\"tr\")\n",
    "            rows = rows[1:11]\n",
    "\n",
    "            for row in rows:\n",
    "                columns = row.find_all(\"td\")\n",
    "                player_name = columns[1].get_text(strip=True)\n",
    "                team = columns[2].get_text(strip=True)\n",
    "                rating = columns[3].get_text(strip=True)\n",
    "                data.append([player_name, team, rating])\n",
    "\n",
    "            if ranking_type == \"batsmen\":\n",
    "                df = pd.DataFrame(data, columns=[\"Batsman\", \"Team\", \"Rating\"])\n",
    "            else:\n",
    "                df = pd.DataFrame(data, columns=[\"Player\", \"Team\", \"Rating\"])\n",
    "            return df\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "womens_odi_teams_url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "womens_odi_batsmen_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "womens_odi_allrounders_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "womens_odi_teams_df = scrape_womens_odi_rankings(womens_odi_teams_url, \"teams\")\n",
    "print(\"Top 10 ODI Teams (Women's Cricket):\\n\", womens_odi_teams_df)\n",
    "womens_odi_batsmen_df = scrape_womens_odi_rankings(womens_odi_batsmen_url, \"batsmen\")\n",
    "print(\"Top 10 ODI Batsmen (Women's Cricket):\\n\", womens_odi_batsmen_df)\n",
    "womens_odi_allrounders_df = scrape_womens_odi_rankings(womens_odi_allrounders_url, \"allrounders\")\n",
    "print(\"Top 10 ODI All-rounders (Women's Cricket):\\n\", womens_odi_allrounders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b835c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "01c848d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m news_links \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m news_container \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPageBuilder-container\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m news_container\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCard-text\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     headline \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCard-title\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     20\u001b[0m     headlines\u001b[38;5;241m.\u001b[39mappend(headline)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    headlines = []\n",
    "    times = []\n",
    "    news_links = []\n",
    "\n",
    "    news_container = soup.find(\"div\", class_=\"PageBuilder-container\")\n",
    "\n",
    "    for article in news_container.find_all(\"div\", class_=\"Card-text\"):\n",
    "        headline = article.find(\"h1\", class_=\"Card-title\").text.strip()\n",
    "        headlines.append(headline)\n",
    "\n",
    "        time = article.find(\"time\", class_=\"Card-time\").text.strip()\n",
    "        times.append(time)\n",
    "\n",
    "        news_link = \"https://www.cnbc.com\" + article.find(\"a\", class_=\"Card-title\")[\"href\"]\n",
    "        news_links.append(news_link)\n",
    "\n",
    "    data = {\n",
    "        \"Headline\": headlines,\n",
    "        \"Time\": times,\n",
    "        \"News Link\": news_links,\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c51d30d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c334d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    paper_titles = []\n",
    "    authors_list = []\n",
    "    published_dates = []\n",
    "    paper_urls = []\n",
    "    article_cards = soup.find_all(\"li\", class_=\"js-article\")\n",
    "    for card in article_cards:\n",
    "        title = card.find(\"a\", class_=\"highwire-cite-title\").text.strip()\n",
    "        paper_titles.append(title)\n",
    "        authors = card.find(\"ul\", class_=\"highwire-article-citation-authors\").text.strip()\n",
    "        authors_list.append(authors)\n",
    "        date = card.find(\"span\", class_=\"highwire-cite-metadata-online-date\").text.strip()\n",
    "        published_dates.append(date)\n",
    "        url = \"https://www.journals.elsevier.com\" + card.find(\"a\", class_=\"highwire-cite-linked-title\")[\"href\"]\n",
    "        paper_urls.append(url)\n",
    "    data = {\n",
    "        \"Paper Title\": paper_titles,\n",
    "        \"Authors\": authors_list,\n",
    "        \"Published Date\": published_dates,\n",
    "        \"Paper URL\": paper_urls,\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635ea8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f17fd331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.dineout.co.in/delhi-restaurants\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    restaurant_names = []\n",
    "    cuisines = []\n",
    "    locations = []\n",
    "    ratings = []\n",
    "    image_urls = []\n",
    "\n",
    "    restaurant_cards = soup.find_all(\"div\", class_=\"dtrestntboxinner\")\n",
    "\n",
    "    for card in restaurant_cards:\n",
    "        name = card.find(\"h2\", class_=\"store-name\").text.strip()\n",
    "        restaurant_names.append(name)\n",
    "\n",
    "        cuisine = card.find(\"div\", class_=\"doubleline\").text.strip()\n",
    "        cuisines.append(cuisine)\n",
    "\n",
    "        location = card.find(\"p\", class_=\"restnt-loc\").text.strip()\n",
    "        locations.append(location)\n",
    "\n",
    "        rating = card.find(\"span\", class_=\"green-box\").text.strip()\n",
    "        ratings.append(rating)\n",
    "\n",
    "        image_url = card.find(\"img\", class_=\"noflicker\")[\"data-src\"]\n",
    "        image_urls.append(image_url)\n",
    "\n",
    "    data = {\n",
    "        \"Restaurant Name\": restaurant_names,\n",
    "        \"Cuisine\": cuisines,\n",
    "        \"Location\": locations,\n",
    "        \"Ratings\": ratings,\n",
    "        \"Image URL\": image_urls,\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
